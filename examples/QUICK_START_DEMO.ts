#!/usr/bin/env tsx

/**
 * üöÄ Quick Start Demo - AI Framework Crawling & Processing
 * 
 * This script demonstrates the complete crawling pipeline:
 * 1. Full website crawling with discovery
 * 2. Manual page addition
 * 3. Vector search across crawled content
 * 
 * Run this script to test the complete framework functionality.
 */

import { config } from 'dotenv';
import { Pool } from 'pg';
import * as path from 'path';

// Load environment variables
config({ path: path.resolve(__dirname, '../.env') });

// Framework imports
import { CrawlingPipelineApplication } from '../packages/ai-framework/src/application/CrawlingPipelineApplication';

// Repositories
import {
  PostgreSQLWebsiteRepository,
  PostgreSQLPageRepository,
  PostgreSQLCrawlSessionRepository,
  PostgreSQLChunkRepository
} from '../packages/ai-framework/src/infrastructure/database';

// Services
import {
  PageDiscoveryService,
  HtmlFetcherService,
  ContentExtractionService,
  TextChunkingService,
  EmbeddingService
} from '../packages/ai-framework/src/domain/services';

// Colors for console output
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m'
};

function log(message: string, color = colors.reset) {
  console.log(`${color}${message}${colors.reset}`);
}

async function quickStartDemo() {
  log('\nüöÄ AI Framework - Crawling & Processing Demo', colors.bright + colors.blue);
  log('=' .repeat(60), colors.blue);

  // Initialize database connection
  const pool = new Pool({
    connectionString: process.env.DATABASE_URL,
  });

  try {
    // Test database connection
    log('\n1Ô∏è‚É£ Testing Database Connection...', colors.yellow);
    const client = await pool.connect();
    await client.query('SELECT NOW()');
    client.release();
    log('‚úÖ Database connection successful', colors.green);

    // Initialize repositories
    log('\n2Ô∏è‚É£ Initializing Framework Components...', colors.yellow);
    const websiteRepository = new PostgreSQLWebsiteRepository(pool);
    const pageRepository = new PostgreSQLPageRepository(pool);
    const crawlSessionRepository = new PostgreSQLCrawlSessionRepository(pool);
    const chunkRepository = new PostgreSQLChunkRepository(pool);

    // Initialize services
    const pageDiscoveryService = new PageDiscoveryService();
    const htmlFetcherService = new HtmlFetcherService();
    const contentExtractionService = new ContentExtractionService();
    const textChunkingService = new TextChunkingService();
    const embeddingService = new EmbeddingService();

    // Initialize pipeline application
    const pipelineApp = new CrawlingPipelineApplication(
      websiteRepository,
      pageRepository,
      crawlSessionRepository,
      chunkRepository,
      pageDiscoveryService,
      htmlFetcherService,
      contentExtractionService,
      textChunkingService,
      embeddingService
    );

    log('‚úÖ Framework components initialized', colors.green);

    // Demo 1: Full Website Crawling
    log('\n3Ô∏è‚É£ Demo 1: Full Website Crawling', colors.yellow);
    log('Target: https://example.com (small, fast demo site)', colors.cyan);
    
    const fullCrawlResult = await pipelineApp.executeFullCrawl({
      websiteUrl: 'https://example.com',
      maxPages: 3,
      maxDepth: 1,
      description: 'Demo website for testing'
    });

    if (fullCrawlResult.success) {
      log('‚úÖ Full crawl completed successfully', colors.green);
      log(`üìä Pages processed: ${fullCrawlResult.summary.pagesProcessed}`, colors.cyan);
      log(`üìÑ Chunks created: ${fullCrawlResult.summary.chunksCreated}`, colors.cyan);
      log(`üßÆ Embeddings generated: ${fullCrawlResult.summary.embeddingsGenerated}`, colors.cyan);
      log(`‚è±Ô∏è Processing time: ${(fullCrawlResult.summary.processingTimeMs / 1000).toFixed(1)}s`, colors.cyan);
      log(`üí∞ Cost: $${fullCrawlResult.summary.totalCost.toFixed(4)}`, colors.cyan);
      log(`üéØ Average quality: ${fullCrawlResult.summary.averageQuality}%`, colors.cyan);
    } else {
      log(`‚ùå Full crawl failed: ${fullCrawlResult.error}`, colors.red);
    }

    // Demo 2: Manual Page Addition
    log('\n4Ô∏è‚É£ Demo 2: Manual Page Addition', colors.yellow);
    log('Adding a different page with high priority', colors.cyan);
    
    try {
      const addPageResult = await pipelineApp.executeAddPage({
        websiteUrl: 'https://httpbin.org',
        pageUrl: 'https://httpbin.org/html',
        title: 'HTTPBin HTML Test Page',
        priority: 95
      });

      if (addPageResult.success) {
        log('‚úÖ Page addition completed successfully', colors.green);
        log(`üìÑ Chunks created: ${addPageResult.summary.chunksCreated}`, colors.cyan);
        log(`üßÆ Embeddings generated: ${addPageResult.summary.embeddingsGenerated}`, colors.cyan);
        log(`‚è±Ô∏è Processing time: ${(addPageResult.summary.processingTimeMs / 1000).toFixed(1)}s`, colors.cyan);
        log(`üí∞ Cost: $${addPageResult.summary.totalCost.toFixed(4)}`, colors.cyan);
      } else {
        log(`‚ùå Page addition failed: ${addPageResult.error}`, colors.red);
      }
    } catch (error: any) {
      log(`‚ùå Page addition error: ${error.message}`, colors.red);
      log('‚ÑπÔ∏è Skipping manual page addition demo', colors.yellow);
    }

    // Demo 3: Vector Search
    log('\n5Ô∏è‚É£ Demo 3: Vector Search Across Crawled Content', colors.yellow);
    
    const searchQuery = "example domain information";
    log(`üîç Searching for: "${searchQuery}"`, colors.cyan);
    
    // Generate embedding for search query
    const { embedding } = await embeddingService.generateEmbedding(searchQuery);
    
    // Search for similar chunks
    const searchResults = await chunkRepository.searchSimilar(embedding, 3);
    
    if (searchResults.length > 0) {
      log(`‚úÖ Found ${searchResults.length} relevant chunks`, colors.green);
      
      searchResults.forEach((chunk, index) => {
        log(`\nüìÑ Result ${index + 1}:`, colors.magenta);
        log(`   üéØ Similarity: ${((chunk.similarity || 0) * 100).toFixed(1)}%`, colors.cyan);
        const content = chunk.content || 'No content available';
        log(`   üìù Content: ${content.substring(0, 150)}${content.length > 150 ? '...' : ''}`, colors.reset);
        log(`   üîó Source: Page ID ${chunk.pageId || 'unknown'}`, colors.cyan);
      });
    } else {
      log('‚ùå No search results found', colors.red);
    }

    // Demo 4: Database Statistics
    log('\n6Ô∏è‚É£ Demo 4: Database Statistics', colors.yellow);
    
    const websiteStats = await pool.query(`
      SELECT 
        COUNT(DISTINCT w.id) as websites,
        COUNT(DISTINCT p.id) as pages,
        COUNT(DISTINCT c.id) as chunks,
        COALESCE(SUM(c.token_count), 0) as total_tokens
      FROM websites w
      LEFT JOIN pages p ON w.id = p.website_id
      LEFT JOIN chunks c ON p.id = c.page_id
    `);
    
    const stats = websiteStats.rows[0];
    log('üìä Current Database Statistics:', colors.magenta);
    log(`   üåê Websites: ${stats.websites}`, colors.cyan);
    log(`   üìÑ Pages: ${stats.pages}`, colors.cyan);
    log(`   üß© Chunks: ${stats.chunks}`, colors.cyan);
    log(`   üìù Total tokens: ${parseInt(stats.total_tokens).toLocaleString()}`, colors.cyan);

    // Demo 5: Recent Activity
    log('\n7Ô∏è‚É£ Demo 5: Recent Crawling Activity', colors.yellow);
    
    const recentSessions = await pool.query(`
      SELECT 
        cs.id,
        w.domain,
        cs.status,
        cs.pages_discovered,
        cs.pages_completed,
        cs.chunks_created,
        cs.completed_at
      FROM crawl_sessions cs
      JOIN websites w ON cs.website_id = w.id
      ORDER BY cs.created_at DESC
      LIMIT 5
    `);
    
    if (recentSessions.rows.length > 0) {
      log('üìà Recent Crawl Sessions:', colors.magenta);
      recentSessions.rows.forEach((session, index) => {
        const completedAt = session.completed_at ? new Date(session.completed_at).toLocaleString() : 'In progress';
        log(`   ${index + 1}. ${session.domain} - ${session.status}`, colors.cyan);
        log(`      üìä ${session.pages_completed}/${session.pages_discovered} pages, ${session.chunks_created} chunks`, colors.reset);
        log(`      ‚è∞ ${completedAt}`, colors.reset);
      });
    } else {
      log('üìà No recent crawl sessions found', colors.cyan);
    }

    log('\nüéâ Demo Complete!', colors.bright + colors.green);
    log('=' .repeat(60), colors.green);
    log('\nüìö Next Steps:', colors.yellow);
    log('1. Check the complete documentation: docs/CRAWLING_COMPLETE_GUIDE.md', colors.reset);
    log('2. Try the API endpoints: /api/crawling/full-crawl', colors.reset);
    log('3. Integrate into your application using the framework components', colors.reset);
    log('4. Explore the example scripts in the examples/ directory', colors.reset);

  } catch (error: any) {
    log(`\n‚ùå Demo failed: ${error.message}`, colors.bright + colors.red);
    log(`Stack trace: ${error.stack}`, colors.red);
  } finally {
    await pool.end();
  }
}

// Run the demo
if (require.main === module) {
  quickStartDemo().catch((error) => {
    console.error('‚ùå Fatal error:', error.message);
    process.exit(1);
  });
}

export { quickStartDemo };
