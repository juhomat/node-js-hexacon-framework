/**
 * API Endpoints Test Script
 * 
 * Tests the new crawling API endpoints:
 * 1. /api/crawling/full-crawl - Complete website crawling pipeline
 * 2. /api/crawling/add-page - Manual page addition pipeline
 * 3. /api/crawling/full-crawl-stream - Streaming crawling with progress
 * 
 * Usage:
 * npx tsx test-api-endpoints.ts --endpoint full-crawl --url https://example.com
 * npx tsx test-api-endpoints.ts --endpoint add-page --website https://example.com --page https://example.com/docs
 * npx tsx test-api-endpoints.ts --endpoint stream --url https://example.com
 */

import fetch from 'node-fetch';

interface CliArgs {
  endpoint?: 'full-crawl' | 'add-page' | 'stream' | 'info';
  url?: string;
  website?: string;
  page?: string;
  maxPages?: number;
  maxDepth?: number;
  priority?: number;
  help?: boolean;
}

function parseArgs(): CliArgs {
  const args = process.argv.slice(2);
  const parsed: CliArgs = {};
  
  for (let i = 0; i < args.length; i++) {
    const arg = args[i];
    const nextArg = args[i + 1];
    
    switch (arg) {
      case '--help': case '-h': 
        parsed.help = true; 
        break;
      case '--endpoint': case '-e': 
        parsed.endpoint = nextArg as any; 
        i++; 
        break;
      case '--url': case '-u': 
        parsed.url = nextArg; 
        i++; 
        break;
      case '--website': case '-w': 
        parsed.website = nextArg; 
        i++; 
        break;
      case '--page': case '-p': 
        parsed.page = nextArg; 
        i++; 
        break;
      case '--max-pages': 
        parsed.maxPages = parseInt(nextArg); 
        i++; 
        break;
      case '--max-depth': 
        parsed.maxDepth = parseInt(nextArg); 
        i++; 
        break;
      case '--priority': 
        parsed.priority = parseInt(nextArg); 
        i++; 
        break;
    }
  }
  
  return parsed;
}

function showHelp() {
  console.log(`
ğŸŒ AI Framework - API Endpoints Test Script

Test the new crawling API endpoints with real requests.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ USAGE

  npx tsx test-api-endpoints.ts [options]

ğŸ“Š OPTIONS

  --endpoint, -e <type>     API endpoint to test:
                            â€¢ full-crawl: Complete website crawling
                            â€¢ add-page: Manual page addition
                            â€¢ stream: Streaming crawling with progress
                            â€¢ info: Show endpoint documentation
                            
  --url, -u <url>          Website URL for full-crawl and stream endpoints
                           Example: https://docs.openai.com
                           
  --website, -w <url>      Website URL for add-page endpoint
                           Example: https://docs.openai.com
                           
  --page, -p <url>         Page URL for add-page endpoint
                           Example: https://docs.openai.com/api-reference
                           
  --max-pages <number>     Maximum pages to crawl (default: 5)
                           Only for full-crawl and stream endpoints
                           
  --max-depth <number>     Maximum crawling depth (default: 1)
                           Only for full-crawl and stream endpoints
                           
  --priority <number>      Page priority 0-100 (default: 80)
                           Only for add-page endpoint
                           
  --help, -h              Show this help message

ğŸš€ EXAMPLES

  # Test full website crawling
  npx tsx test-api-endpoints.ts -e full-crawl -u https://example.com --max-pages 3

  # Test manual page addition
  npx tsx test-api-endpoints.ts -e add-page -w https://docs.openai.com -p https://docs.openai.com/api-reference/chat

  # Test streaming crawling
  npx tsx test-api-endpoints.ts -e stream -u https://example.com --max-pages 2

  # Get endpoint information
  npx tsx test-api-endpoints.ts -e info

ğŸ’¡ PREREQUISITES

  1. Admin application must be running (npm run dev in admin_application/)
  2. Database must be set up with crawling schema
  3. OPENAI_API_KEY must be configured
  4. Environment variables properly set

ğŸ“Š EXPECTED BEHAVIOR

  Full Crawl:
    â€¢ Discovers pages from sitemap/navigation
    â€¢ Extracts content from each page
    â€¢ Creates text chunks (300-400 tokens)
    â€¢ Generates OpenAI embeddings
    â€¢ Stores everything in PostgreSQL

  Add Page:
    â€¢ Adds specific page to website
    â€¢ Extracts content from that page
    â€¢ Creates chunks and embeddings
    â€¢ Stores in database

  Stream:
    â€¢ Same as full crawl but with real-time progress
    â€¢ Shows progress for each pipeline stage
    â€¢ Uses Server-Sent Events

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ This script tests the complete end-to-end pipeline API endpoints.
   Make sure the admin application is running on localhost:3000.
`);
}

async function testFullCrawl(args: CliArgs) {
  if (!args.url) {
    console.error('âŒ URL is required for full-crawl endpoint');
    console.log('ğŸ’¡ Example: npx tsx test-api-endpoints.ts -e full-crawl -u https://example.com');
    return;
  }

  console.log('ğŸš€ Testing Full Crawl Endpoint');
  console.log('============================================');
  console.log(`ğŸŒ URL: ${args.url}`);
  console.log(`ğŸ“„ Max Pages: ${args.maxPages || 5}`);
  console.log(`ğŸ“Š Max Depth: ${args.maxDepth || 1}`);
  console.log();

  try {
    const startTime = Date.now();
    
    const response = await fetch('http://localhost:3000/api/crawling/full-crawl', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        websiteUrl: args.url,
        maxPages: args.maxPages || 5,
        maxDepth: args.maxDepth || 1,
        description: `Test crawl from API test script`,
        sessionMetadata: {
          testScript: true,
          testTimestamp: new Date().toISOString()
        }
      }),
    });

    const result = await response.json();
    const duration = Date.now() - startTime;

    if (result.success) {
      console.log('âœ… Full Crawl Completed Successfully!');
      console.log('============================================');
      console.log(`â±ï¸  Duration: ${duration}ms`);
      console.log(`ğŸŒ Website: ${result.website.domain}`);
      console.log(`ğŸ“Š Pages Discovered: ${result.summary.pagesDiscovered}`);
      console.log(`ğŸ“„ Pages Processed: ${result.summary.pagesProcessed}`);
      console.log(`ğŸ§® Chunks Created: ${result.summary.chunksCreated}`);
      console.log(`ğŸ”¢ Embeddings Generated: ${result.summary.embeddingsGenerated}`);
      console.log(`ğŸ’° Total Cost: $${result.summary.totalCost.toFixed(4)}`);
      console.log(`ğŸ“ˆ Average Quality: ${result.summary.averageQuality.toFixed(1)}/100`);
      console.log();
      
      if (result.pages.length > 0) {
        console.log('ğŸ“‹ Processed Pages:');
        result.pages.slice(0, 3).forEach((page: any, index: number) => {
          console.log(`  ${index + 1}. ${page.title}`);
          console.log(`     URL: ${page.url}`);
          console.log(`     Status: ${page.status === 'completed' ? 'âœ…' : 'âŒ'} ${page.status}`);
          console.log(`     Chunks: ${page.chunksCreated}, Embeddings: ${page.embeddingsGenerated}`);
          console.log(`     Quality: ${page.quality}/100, Tokens: ${page.tokenCount}`);
          console.log();
        });
        
        if (result.pages.length > 3) {
          console.log(`     ... and ${result.pages.length - 3} more pages`);
        }
      }
      
      console.log(`ğŸ’¬ ${result.message}`);
      
    } else {
      console.error('âŒ Full Crawl Failed');
      console.error('============================================');
      console.error(`â±ï¸  Duration: ${duration}ms`);
      console.error(`ğŸš¨ Error: ${result.error}`);
      console.error(`ğŸ’¬ Message: ${result.message}`);
    }

  } catch (error: any) {
    console.error('âŒ API Request Failed');
    console.error('============================================');
    console.error(`ğŸš¨ Error: ${error.message}`);
    console.error('ğŸ’¡ Make sure the admin application is running on localhost:3000');
  }
}

async function testAddPage(args: CliArgs) {
  if (!args.website || !args.page) {
    console.error('âŒ Both website and page URLs are required for add-page endpoint');
    console.log('ğŸ’¡ Example: npx tsx test-api-endpoints.ts -e add-page -w https://example.com -p https://example.com/docs');
    return;
  }

  console.log('ğŸ“„ Testing Add Page Endpoint');
  console.log('============================================');
  console.log(`ğŸŒ Website: ${args.website}`);
  console.log(`ğŸ“„ Page: ${args.page}`);
  console.log(`ğŸ¯ Priority: ${args.priority || 80}`);
  console.log();

  try {
    const startTime = Date.now();
    
    const response = await fetch('http://localhost:3000/api/crawling/add-page', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        websiteUrl: args.website,
        pageUrl: args.page,
        title: 'Test Page from API Script',
        description: 'Page added via API test script',
        priority: args.priority || 80
      }),
    });

    const result = await response.json();
    const duration = Date.now() - startTime;

    if (result.success) {
      console.log('âœ… Page Addition Completed Successfully!');
      console.log('============================================');
      console.log(`â±ï¸  Duration: ${duration}ms`);
      console.log(`ğŸŒ Website: ${result.website.domain}`);
      console.log(`ğŸ“„ Page Added: ${result.pages[0]?.title}`);
      console.log(`ğŸ§® Chunks Created: ${result.summary.chunksCreated}`);
      console.log(`ğŸ”¢ Embeddings Generated: ${result.summary.embeddingsGenerated}`);
      console.log(`ğŸ’° Total Cost: $${result.summary.totalCost.toFixed(4)}`);
      console.log(`ğŸ“ˆ Quality: ${result.summary.averageQuality.toFixed(1)}/100`);
      console.log();
      
      const page = result.pages[0];
      if (page) {
        console.log('ğŸ“‹ Page Details:');
        console.log(`   Title: ${page.title}`);
        console.log(`   URL: ${page.url}`);
        console.log(`   Status: ${page.status === 'completed' ? 'âœ…' : 'âŒ'} ${page.status}`);
        console.log(`   Chunks: ${page.chunksCreated}`);
        console.log(`   Embeddings: ${page.embeddingsGenerated}`);
        console.log(`   Quality: ${page.quality}/100`);
        console.log(`   Tokens: ${page.tokenCount}`);
        console.log();
      }
      
      console.log(`ğŸ’¬ ${result.message}`);
      
    } else {
      console.error('âŒ Page Addition Failed');
      console.error('============================================');
      console.error(`â±ï¸  Duration: ${duration}ms`);
      console.error(`ğŸš¨ Error: ${result.error}`);
      console.error(`ğŸ’¬ Message: ${result.message}`);
    }

  } catch (error: any) {
    console.error('âŒ API Request Failed');
    console.error('============================================');
    console.error(`ğŸš¨ Error: ${error.message}`);
    console.error('ğŸ’¡ Make sure the admin application is running on localhost:3000');
  }
}

async function testStreamingCrawl(args: CliArgs) {
  if (!args.url) {
    console.error('âŒ URL is required for streaming endpoint');
    console.log('ğŸ’¡ Example: npx tsx test-api-endpoints.ts -e stream -u https://example.com');
    return;
  }

  console.log('ğŸŒŠ Testing Streaming Crawl Endpoint');
  console.log('============================================');
  console.log(`ğŸŒ URL: ${args.url}`);
  console.log(`ğŸ“„ Max Pages: ${args.maxPages || 3}`);
  console.log(`ğŸ“Š Max Depth: ${args.maxDepth || 1}`);
  console.log('ğŸ”„ Listening for real-time updates...');
  console.log();

  try {
    const response = await fetch('http://localhost:3000/api/crawling/full-crawl-stream', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        websiteUrl: args.url,
        maxPages: args.maxPages || 3,
        maxDepth: args.maxDepth || 1,
        description: `Test streaming crawl from API test script`,
        sessionMetadata: {
          testScript: true,
          streaming: true,
          testTimestamp: new Date().toISOString()
        }
      }),
    });

    if (!response.body) {
      throw new Error('No response body received');
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    let buffer = '';
    
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      buffer += decoder.decode(value, { stream: true });
      
      const lines = buffer.split('\n');
      buffer = lines.pop() || ''; // Keep incomplete line in buffer
      
      for (const line of lines) {
        if (line.startsWith('data: ')) {
          try {
            const data = JSON.parse(line.slice(6));
            
            switch (data.type) {
              case 'connected':
                console.log('ğŸ”— Connected to streaming pipeline');
                break;
                
              case 'progress':
                const progressBar = 'â–ˆ'.repeat(Math.floor(data.progress / 5)) + 'â–‘'.repeat(20 - Math.floor(data.progress / 5));
                console.log(`ğŸ“Š [${progressBar}] ${data.progress.toFixed(1)}% - ${data.stage}: ${data.message}`);
                
                if (data.details.pagesDiscovered) {
                  console.log(`   ğŸ“„ Pages: ${data.details.pagesProcessed || 0}/${data.details.pagesDiscovered} processed`);
                }
                if (data.details.chunksCreated) {
                  console.log(`   ğŸ§® Chunks: ${data.details.chunksCreated}, Embeddings: ${data.details.embeddingsGenerated || 0}`);
                }
                if (data.details.totalCost) {
                  console.log(`   ğŸ’° Cost: $${data.details.totalCost.toFixed(4)}`);
                }
                console.log();
                break;
                
              case 'result':
                console.log('âœ… Streaming Crawl Completed Successfully!');
                console.log('============================================');
                console.log(`ğŸŒ Website: ${data.website.domain}`);
                console.log(`ğŸ“Š Pages Discovered: ${data.summary.pagesDiscovered}`);
                console.log(`ğŸ“„ Pages Processed: ${data.summary.pagesProcessed}`);
                console.log(`ğŸ§® Chunks Created: ${data.summary.chunksCreated}`);
                console.log(`ğŸ”¢ Embeddings Generated: ${data.summary.embeddingsGenerated}`);
                console.log(`â±ï¸  Duration: ${data.summary.processingTimeMs}ms`);
                console.log(`ğŸ’° Total Cost: $${data.summary.totalCost.toFixed(4)}`);
                console.log(`ğŸ“ˆ Average Quality: ${data.summary.averageQuality.toFixed(1)}/100`);
                console.log();
                console.log(`ğŸ’¬ ${data.message}`);
                break;
                
              case 'error':
                console.error('âŒ Streaming Crawl Failed');
                console.error('============================================');
                console.error(`ğŸš¨ Error: ${data.error}`);
                console.error(`ğŸ’¬ Message: ${data.message}`);
                console.error(`ğŸ• Time: ${data.timestamp}`);
                break;
            }
          } catch (parseError) {
            console.error('âŒ Failed to parse SSE data:', line);
          }
        }
      }
    }

  } catch (error: any) {
    console.error('âŒ Streaming Request Failed');
    console.error('============================================');
    console.error(`ğŸš¨ Error: ${error.message}`);
    console.error('ğŸ’¡ Make sure the admin application is running on localhost:3000');
  }
}

async function showEndpointInfo() {
  console.log('ğŸ“š API Endpoints Information');
  console.log('============================================');
  
  const endpoints = [
    'full-crawl',
    'add-page',
    'full-crawl-stream'
  ];
  
  for (const endpoint of endpoints) {
    try {
      const response = await fetch(`http://localhost:3000/api/crawling/${endpoint}`, {
        method: 'GET'
      });
      
      const info = await response.json();
      
      console.log(`\nğŸ”— ${info.endpoint}`);
      console.log(`ğŸ“ ${info.description}`);
      console.log(`ğŸ“Š Method: ${info.method}`);
      
      if (info.examples?.basic) {
        console.log(`ğŸ’¡ Example:`, JSON.stringify(info.examples.basic, null, 2));
      }
      
    } catch (error) {
      console.error(`âŒ Failed to get info for ${endpoint}:`, error);
    }
  }
}

async function main() {
  const args = parseArgs();

  if (args.help) {
    showHelp();
    process.exit(0);
  }

  if (!args.endpoint) {
    console.log('âš ï¸ No endpoint specified');
    console.log();
    console.log('ğŸ’¡ Available endpoints:');
    console.log('   --endpoint full-crawl    Complete website crawling');
    console.log('   --endpoint add-page      Manual page addition');
    console.log('   --endpoint stream        Streaming crawling with progress');
    console.log('   --endpoint info          Show endpoint documentation');
    console.log();
    console.log('   Use --help for detailed usage information');
    process.exit(1);
  }

  switch (args.endpoint) {
    case 'full-crawl':
      await testFullCrawl(args);
      break;
    case 'add-page':
      await testAddPage(args);
      break;
    case 'stream':
      await testStreamingCrawl(args);
      break;
    case 'info':
      await showEndpointInfo();
      break;
    default:
      console.error(`âŒ Unknown endpoint: ${args.endpoint}`);
      console.log('ğŸ’¡ Available: full-crawl, add-page, stream, info');
      process.exit(1);
  }
}

main().catch(console.error);
