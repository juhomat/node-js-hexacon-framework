# AI Framework

A modular, reusable Node.js framework using hexagonal architecture (ports and adapters) for rapid development of AI-powered web applications.

## Features

- ü§ñ **AI Chat & Image Generation** - OpenAI integration for chat and DALL-E image generation
- üß† **RAG (Retrieval-Augmented Generation)** - Full pipeline for document processing and intelligent retrieval
- üï∑Ô∏è **Web Scraping & Embedding** - Real-time website crawling with vector embeddings for RAG
- üîê **Authentication** - Firebase Auth with email and Google login support
- üí≥ **Payments** - Stripe integration for one-time and recurring billing
- üìä **History Logging** - PostgreSQL with pgvector for comprehensive activity tracking
- üîß **Modular Architecture** - Easily extendable and replaceable service adapters
- üñ•Ô∏è **Admin Interface** - Built-in web UI for configuration management and component testing

## Architecture

Built on **Hexagonal Architecture** principles:

- **Domain Layer**: Core business logic, entities, and use cases
- **Application Layer**: Application services and orchestration
- **Infrastructure Layer**: External service adapters (OpenAI, Stripe, Firebase, etc.)
- **Presentation Layer**: API controllers and framework interfaces

## Quick Start

### Prerequisites

- Node.js >= 18.0.0
- PostgreSQL database
- OpenAI API key

### Installation

```bash
# Install dependencies
npm install

# Build the framework
npm run build

# Run tests
npm test
```

### Database Setup

```bash
# Create database
createdb ai_framework_db

# Install required extensions
psql ai_framework_db -c "CREATE EXTENSION IF NOT EXISTS vector;"

# Setup complete schema (base + crawling)
cd packages/ai-framework
npm run setup-db

# Or setup individually:
# psql ai_framework_db -f src/infrastructure/database/schema.sql
# npm run setup-crawling-schema
```

> üìñ **For complete setup instructions, see [Complete Crawling Guide](docs/CRAWLING_COMPLETE_GUIDE.md)**

### Basic Usage

```typescript
import { Pool } from 'pg';
import { 
  OpenAIAdapter, 
  PostgreSQLChatRepository, 
  PostgreSQLMessageRepository,
  ChatApplication 
} from '@ai-framework/core';

// Initialize dependencies
const dbPool = new Pool({
  connectionString: process.env.DATABASE_URL
});

const openaiAdapter = new OpenAIAdapter(process.env.OPENAI_API_KEY, {
  defaultModel: 'gpt-4o'
});

const chatRepository = new PostgreSQLChatRepository(dbPool);
const messageRepository = new PostgreSQLMessageRepository(dbPool);

// Create application service
const chatApp = new ChatApplication(
  chatRepository,
  messageRepository,
  openaiAdapter
);

// Create a chat and send a message
async function example() {
  const { chat } = await chatApp.createChat({
    userId: 'user123',
    title: 'My Chat',
    model: 'gpt-4o'
  });

  const response = await chatApp.sendMessage({
    chatId: chat.id,
    userId: 'user123',
    content: 'Hello, AI!'
  });

  console.log(response.assistantMessage.content);
}
```

### Crawling & RAG Usage

```typescript
import { Pool } from 'pg';
import { 
  CrawlingPipelineApplication,
  PostgreSQLWebsiteRepository,
  PostgreSQLPageRepository,
  PostgreSQLCrawlSessionRepository,
  PostgreSQLChunkRepository,
  PageDiscoveryService,
  HtmlFetcherService,
  ContentExtractionService,
  TextChunkingService,
  EmbeddingService
} from '@ai-framework/core';

// Initialize database and services
const dbPool = new Pool({ connectionString: process.env.DATABASE_URL });

const crawlingPipeline = new CrawlingPipelineApplication(
  new PostgreSQLWebsiteRepository(dbPool),
  new PostgreSQLPageRepository(dbPool),
  new PostgreSQLCrawlSessionRepository(dbPool),
  new PostgreSQLChunkRepository(dbPool),
  new PageDiscoveryService(),
  new HtmlFetcherService(),
  new ContentExtractionService(),
  new TextChunkingService(),
  new EmbeddingService()
);

// Complete website crawling pipeline
async function crawlWebsite() {
  const result = await crawlingPipeline.executeFullCrawl({
    websiteUrl: 'https://docs.openai.com',
    maxPages: 10,    // Crawl up to 10 pages
    maxDepth: 2      // Go 2 levels deep
  });

  console.log(`‚úÖ Processed ${result.summary.pagesProcessed} pages`);
  console.log(`üìÑ Created ${result.summary.chunksCreated} chunks`);
  console.log(`üßÆ Generated ${result.summary.embeddingsGenerated} embeddings`);
  console.log(`üí∞ Cost: $${result.summary.totalCost.toFixed(4)}`);
}

// Add specific page manually
async function addPage() {
  const result = await crawlingPipeline.executeAddPage({
    websiteUrl: 'https://docs.openai.com',
    pageUrl: 'https://docs.openai.com/api-reference/chat',
    priority: 95
  });

  console.log(`üìÑ Added page with ${result.summary.chunksCreated} chunks`);
}

// Search crawled content
async function searchContent() {
  const chunkRepository = new PostgreSQLChunkRepository(dbPool);
  const embeddingService = new EmbeddingService();
  
  const { embedding } = await embeddingService.generateEmbedding(
    "How do I use the OpenAI chat API?"
  );
  
  const results = await chunkRepository.searchSimilar(embedding, 5);
  
  results.forEach(chunk => {
    console.log(`üìä Similarity: ${chunk.similarity.toFixed(3)}`);
    console.log(`üìù Content: ${chunk.content.substring(0, 100)}...`);
  });
}
```

### Development Scripts

```bash
npm run build         # Build TypeScript to JavaScript
npm run build:watch   # Build in watch mode
npm run dev          # Development server with hot reload
npm run test         # Run test suite
npm run test:watch   # Run tests in watch mode
npm run test:coverage # Run tests with coverage report
npm run lint         # Lint source code
npm run lint:fix     # Fix linting issues
npm run format       # Format code with Prettier
npm run clean        # Clean build artifacts
```

## Project Integration

This framework is designed to be integrated into projects using Git subtree:

### Adding to a Project

```bash
git subtree add --prefix=packages/ai-framework https://github.com/your-org/ai-framework main --squash
```

### Updating Framework

```bash
git subtree pull --prefix=packages/ai-framework https://github.com/your-org/ai-framework main --squash
```

## Configuration

Create a `.env` file with the following variables:

```env
# OpenAI
OPENAI_API_KEY=your_openai_api_key

# Firebase
FIREBASE_PROJECT_ID=your_firebase_project_id
FIREBASE_PRIVATE_KEY=your_firebase_private_key
FIREBASE_CLIENT_EMAIL=your_firebase_client_email

# Stripe
STRIPE_SECRET_KEY=your_stripe_secret_key
STRIPE_WEBHOOK_SECRET=your_stripe_webhook_secret

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# Crawling (Optional)
CRAWLING_MAX_PAGES=10      # Default pages per crawl session
CRAWLING_MAX_DEPTH=1       # Default crawling depth
CRAWLING_DELAY_MS=1000     # Delay between requests
```

## üìö Documentation

- **[Complete Crawling Guide](docs/CRAWLING_COMPLETE_GUIDE.md)** - üìñ **Start Here!** Complete integration guide
- **[Crawling & RAG System](docs/crawling-system.md)** - Technical architecture overview
- **[Environment Setup](docs/environment-setup.md)** - Detailed setup instructions
- **[Admin Interface Plan](docs/admin-interface-plan.md)** - Admin interface documentation

## Tech Stack

- **Runtime**: Node.js with TypeScript
- **AI Services**: OpenAI (GPT, DALL-E, Embeddings)
- **Database**: PostgreSQL with pgvector
- **Authentication**: Firebase Auth
- **Payments**: Stripe
- **Web Scraping**: Cheerio, Puppeteer
- **Testing**: Jest
- **Code Quality**: ESLint, Prettier

## License

MIT
